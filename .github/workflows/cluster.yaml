name: _Cluster_Management

on:
  workflow_dispatch:
    inputs:
      provider:
        description: "Select the cloud provider"
        required: true
        type: choice
        options:
          - aws
          - exoscale

      config:
        description: "Select the .env file to use"
        required: true
        type: choice
        options:
          - hello-k8s-ca-central-1.env
          - hello-k8s-ch-gva-2.env
          - kubyterlab-llm-ca-central-1.env
          - model-server-ca-central-1.env
          - model-server-ch-gva-2.env
          - personal-cloud-ch-gva-2.env
          - eberron-llm.env

      action:
        description: "Action to perform"
        required: true
        type: choice
        options:
          - provision
          - teardown
          - test
        default: provision

  workflow_call:
    inputs:
      provider:
        description: "Select the cloud provider"
        required: true
        type: string

      config:
        description: "Select the .env file to use"
        required: true
        type: string

      action:
        description: "Action to perform"
        required: true
        type: string

jobs:
  provision-teardown:
    if: inputs.action != 'test'
    env:
      PULUMI_CONFIG_PASSPHRASE: ${{ secrets.PULUMI_PASSPHRASE }}
      EXOSCALE_API_KEY: ${{ secrets.EXOSCALE_API_KEY }}
      EXOSCALE_API_SECRET: ${{ secrets.EXOSCALE_API_SECRET }}
      PULUMI_BACKEND_URL: file://${{ github.workspace }}/.pulumi-state
      PYTHONPATH: ${{ github.workspace }}
    name: ${{ inputs.action == 'provision' && 'Provision Cluster' || 'Teardown Cluster' }}
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Show platform uname
        run: uname -a

      - name: Install dependencies
        run: |
          pip install -r pulumi/${{ inputs.provider }}/requirements.txt

      - name: Install latest Pulumi CLI
        run: |
          pulumi version

      - name: Load the config
        run: |
          echo "Loading env file: ${{ inputs.config }}"
          set -a
          source "configs/${{ inputs.provider }}/${{ inputs.config }}"
          set +a
          grep -v '^#' "configs/${{ inputs.provider }}/${{ inputs.config }}" | grep -E '^[A-Za-z_][A-Za-z0-9_]*=' >> $GITHUB_ENV

      - name: Create AWS credentials secret
        if: github.event.inputs.provider == 'aws'
        run: |
          mkdir -p tmp
          printf "[default]\naws_access_key_id=%s\naws_secret_access_key=%s\nregion=%s\n" \
            "${{ secrets.AWS_ACCESS_KEY_ID }}" \
            "${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            "${{ env.REGION }}" > tmp/credentials

          AWS_SHARED_CREDENTIALS_FILE=tmp/credentials aws sts get-caller-identity
          ACCOUNT_ID=$(AWS_SHARED_CREDENTIALS_FILE=tmp/credentials aws sts get-caller-identity --query Account --output text)
          echo "AWS_ACCOUNT_ID=$ACCOUNT_ID" >> $GITHUB_ENV

      - name: Configure AWS credentials
        if: github.event.inputs.provider == 'aws'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ env.AWS_ACCOUNT_ID }}:role/github-actions-iac
          aws-region: ${{ env.REGION }}

      - name: Configure Pulumi
        run: |
          echo "Configuring Pulumi..."
          pulumi version
          mkdir -p ${{ github.workspace }}/.pulumi-state
          pulumi login file://${{ github.workspace }}/.pulumi-state

      - name: Initialize Pulumi
        working-directory: pulumi/${{ inputs.provider }}
        run: |
          pulumi stack init dev --non-interactive || echo "Stack dev already exists"

      - name: List stacks
        working-directory: pulumi/${{ inputs.provider }}
        run: |
          pulumi stack ls

      - name: Select stack
        working-directory: pulumi/${{ inputs.provider }}
        run: |
          pulumi stack select dev --non-interactive

      - name: Pulumi Preview
        if: ${{ inputs.action == 'provision' }}
        working-directory: pulumi/${{ inputs.provider }}
        run: |
          pulumi preview

      - name: Pulumi Provision
        id: provision
        if: ${{ inputs.action == 'provision' }}
        working-directory: pulumi/${{ inputs.provider }}
        run: |
          pulumi up --yes --non-interactive

      # The following step is necessary because we install load balancers through helm charts, so Pulumi cannot handle them.
      - name: Teardown Load Balancers
        if: ${{ inputs.action == 'teardown' && inputs.provider == 'aws' }}
        run: |
          python scripts/${{ inputs.provider }}/cluster/teardown_load_balancer.py

      - name: Pulumi Teardown
        id: teardown
        if: ${{ inputs.action == 'teardown' }}
        working-directory: pulumi/${{ inputs.provider }}
        run: |
          pulumi cancel --yes --non-interactive || echo "No ongoing update to cancel"
          pulumi destroy --yes --non-interactive

      - name: Commit Pulumi state
        if: always()
        run: |
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git config --global user.name "github-actions[bot]"
          git add .pulumi-state
          git commit -m "Update Pulumi state after ${{ inputs.action }}" || echo "No changes to commit"

          # Try to pull and push with retry logic for concurrent updates
          MAX_RETRIES=5
          RETRY_COUNT=0
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            git pull --no-rebase --no-edit && git push && break
            RETRY_COUNT=$((RETRY_COUNT + 1))
            echo "Pull/push failed, retry $RETRY_COUNT of $MAX_RETRIES"
            sleep $((RETRY_COUNT * 2))
          done

          if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
            echo "Failed to push after $MAX_RETRIES attempts"
            exit 1
          fi

      - name: Export Pulumi outputs
        if: steps.provision.conclusion == 'success'
        working-directory: pulumi/${{ inputs.provider }}
        run: |
          pulumi stack output --show-secrets --json > ${{ env.CLUSTER_NAME}}.json

      - name: Upload Pulumi outputs artifact
        if: steps.provision.conclusion == 'success'
        uses: actions/upload-artifact@v4
        with:
          name: pulumi-${{ inputs.provider }}-${{ env.CLUSTER_NAME}}
          path: pulumi/${{ inputs.provider }}/${{ env.CLUSTER_NAME}}.json

      - name: Export Kubeconfig
        if: steps.provision.conclusion == 'success'
        run: |
          jq -r '.kubeconfig' pulumi/${{ inputs.provider }}/${{ env.CLUSTER_NAME}}.json > kubeconfig.yaml

      - name: Debug Kubernetes connectivity
        if: steps.provision.conclusion == 'success'
        run: |
          echo "Attempting to connect to Kubernetes cluster..."
          # Install kubectl
          curl -LO "https://dl.k8s.io/release/stable.txt"
          KUBECTL_VERSION=$(cat stable.txt)
          curl -LO "https://dl.k8s.io/release/$KUBECTL_VERSION/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

          # Use the generated kubeconfig file
          export KUBECONFIG=kubeconfig.yaml

          # Test kubectl commands
          echo "Getting cluster info:"
          kubectl --kubeconfig=kubeconfig.yaml cluster-info

          echo "Getting nodes:"
          kubectl --kubeconfig=kubeconfig.yaml get nodes -o wide

          echo "Getting all namespaces:"
          kubectl --kubeconfig=kubeconfig.yaml get namespaces

          echo "Getting all pods across all namespaces:"
          kubectl --kubeconfig=kubeconfig.yaml get pods --all-namespaces

      - name: Tag subnets
        if: steps.provision.conclusion == 'success' && github.event.inputs.provider == 'aws'
        run: |
          python scripts/aws/cluster/tag_subnets.py pulumi/${{ inputs.provider }}/${{ env.CLUSTER_NAME}}.json

  test:
    name: Test Cluster
    if: |
      always() &&
      (inputs.action == 'provision' || inputs.action == 'test') &&
      (inputs.action == 'test' || needs.provision-teardown.result == 'success')
    needs: [provision-teardown]
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      actions: read
      contents: read

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Load the config
        id: load-config
        run: |
          echo "Loading env file: ${{ inputs.config }}"
          set -a
          source "configs/${{ inputs.provider }}/${{ inputs.config }}"
          set +a
          grep -v '^#' "configs/${{ inputs.provider }}/${{ inputs.config }}" | grep -E '^[A-Za-z_][A-Za-z0-9_]*=' >> $GITHUB_ENV

          # Export CLUSTER_NAME for artifact download
          CLUSTER_NAME=$(grep '^CLUSTER_NAME=' "configs/${{ inputs.provider }}/${{ inputs.config }}" | cut -d'=' -f2)
          echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT

      - name: Create AWS credentials secret
        if: github.event.inputs.provider == 'aws'
        run: |
          mkdir -p tmp
          printf "[default]\naws_access_key_id=%s\naws_secret_access_key=%s\nregion=%s\n" \
            "${{ secrets.AWS_ACCESS_KEY_ID }}" \
            "${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            "${{ env.REGION }}" > tmp/credentials

          AWS_SHARED_CREDENTIALS_FILE=tmp/credentials aws sts get-caller-identity
          ACCOUNT_ID=$(AWS_SHARED_CREDENTIALS_FILE=tmp/credentials aws sts get-caller-identity --query Account --output text)
          echo "AWS_ACCOUNT_ID=$ACCOUNT_ID" >> $GITHUB_ENV

      - name: Configure AWS credentials
        if: github.event.inputs.provider == 'aws'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ env.AWS_ACCOUNT_ID }}:role/github-actions-iac
          aws-region: ${{ env.REGION }}

      - name: Download Kubeconfig artifact (from current run)
        if: inputs.action == 'provision'
        uses: actions/download-artifact@v4
        with:
          name: pulumi-${{ inputs.provider }}-${{ steps.load-config.outputs.cluster_name }}

      - name: Download Kubeconfig artifact (from previous run)
        if: inputs.action == 'test'
        uses: dawidd6/action-download-artifact@v6
        with:
          name: pulumi-${{ inputs.provider }}-${{ steps.load-config.outputs.cluster_name }}
          workflow: cluster.yaml
          workflow_conclusion: success
          branch: ${{ github.ref_name }}
          search_artifacts: true
          if_no_artifact_found: fail

      - name: Convert Kubeconfig artifact
        run: |
          cat ${{ steps.load-config.outputs.cluster_name }}.json | jq -r '.kubeconfig' > kubeconfig.yaml

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/stable.txt"
          KUBECTL_VERSION=$(cat stable.txt)
          curl -LO "https://dl.k8s.io/release/$KUBECTL_VERSION/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: Test cluster nodes
        run: |
          export KUBECONFIG=kubeconfig.yaml

          cat kubeconfig.yaml

          echo "Expected nodes: DEFAULT_NODE_COUNT=${{ env.DEFAULT_NODE_COUNT }}, GPU_NODE_COUNT=${{ env.GPU_NODE_COUNT }}"

          # Calculate expected node count
          DEFAULT_NODES=${{ env.DEFAULT_NODE_COUNT }}
          GPU_NODES=${{ env.GPU_NODE_COUNT }}
          EXPECTED_NODES=$((DEFAULT_NODES + GPU_NODES))

          echo "Total expected nodes: $EXPECTED_NODES"

          # Get actual node count
          ACTUAL_NODES=$(kubectl get nodes --no-headers | wc -l)
          echo "Actual nodes in cluster: $ACTUAL_NODES"

          # Display node details
          echo "Node details:"
          kubectl get nodes -o wide
          KUBECONFIG=kubeconfig.yaml kubectl get nodes -o wide

          # Verify node count matches
          if [ "$ACTUAL_NODES" -eq "$EXPECTED_NODES" ]; then
            echo "✅ SUCCESS: Node count matches! Expected: $EXPECTED_NODES, Actual: $ACTUAL_NODES"
          else
            echo "❌ FAILURE: Node count mismatch! Expected: $EXPECTED_NODES, Actual: $ACTUAL_NODES"
            exit 1
          fi
